---
permalink: /
title: "About me"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

Hi! I'm Jingmin Wei, a second-year M.S. student studying at Univ. of Southern California, majoring in [Computer Science](https://www.cs.usc.edu/). I'm very fortunate to work with [Prof. Ram Nevatia](https://sites.usc.edu/iris-cvlab/professor-ram-nevatia/) and [Dr. Zhaoheng Zheng](https://zhaohengz.github.io/) at [IRIS Computer Vision Lab](https://sites.usc.edu/iris-cvlab/). My research interest lines in the area of <strong>stability and efficiency<strong>, including Vision Robust Representation (Adversarial Learning) and Parameter-efficient Learning (Multi-modal Adapter, Prompt Learning).

Prior to USC, I obtained my B.E. degree at the [School of Artificial Intelligence and Automation](http://english.aia.hust.edu.cn/), Huazhong Univ. of Science and Technology. I worked as an undergrad research assistant, advised by [Prof. Sheng Zhong](http://english.aia.hust.edu.cn/info/1030/1347.htm), focusing on research about Facial Language Analysis and Understanding.

Feel free to contact me: jingminw@usc.edu, or through [LinkedIn](https://www.linkedin.com/in/jingminwei/).

<font color="#ff0000">I am graduating in 2023/early 2024 and now actively looking for full-time positions in computer vision, machine learning, and artificial intelligence. Please feel free to get in touch if there are any opportunities!</font>

<h1 id="publications"> Selected Publications (<a href="/publications/">Full List</a>)</h1>

<p><u>CAILA: Concept-Aware Intra-Layer Adapters for Compositional Zero-Shot Learning</u><br>
<strong>Zhaoheng Zheng</strong>, Haidong Zhu, and Ram Nevatia
<br> WACV 2024<br>
<a href="https://arxiv.org/abs/2305.16681" class="btn btn--success">Paper</a></p>

<p><u>MoMo: A shared encoder Model for text, image and multi-Modal representations</u><br>
Rakesh Chada, <strong>Zhaoheng Zheng</strong>, and Pradeep Natarajan
<br> arXiv Preprint<br>
<a href="https://arxiv.org/abs/2304.05523" class="btn btn--success">Paper</a></p>

<p><u>PatchZero: Defending against Adversarial Patch Attacks by Detecting and Zeroing the Patch</u><br>
Ke Xu*, Yao Xiao*, <strong>Zhaoheng Zheng</strong>, Kaijie Cai, and Ram Nevatia
<br> WACV 2023<br>
<a href="https://arxiv.org/abs/2207.01795" class="btn btn--success">Paper</a></p>
<!-- <a href="https://github.com/TheShadow29/VidSitu" class="btn btn--warning">Code</a>
; <a href="https://vidsitu.org/" class="btn btn--danger">Website</a> -->

<p><u>FashionVLP: Vision Language Transformer for Fashion Retrieval with Feedback</u><br>
Sonam Goenka*, <strong>Zhaoheng Zheng*</strong>, Ayush Jaiswal, Rakesh Chada, Yue Wu, Pradeep Natarajan, and Varsha Hedau
<br> CVPR 2022<br>
<a href="https://www.amazon.science/publications/fashionvlp-vision-language-transformer-for-fashion-retrieval-with-feedback" class="btn btn--success">Paper</a></p>

<p><u>Improving Object Detection and Attribute Recognition by Feature Entanglement Reduction</u><br>
<strong>Zhaoheng Zheng</strong>, Arka Sadhu, and Ram Nevatia
<br> ICIP 2021<br>
<a href="https://arxiv.org/abs/2108.11501" class="btn btn--success">Paper</a></p>

<p><u>Image Based Cloth Changing System</u><br>
<strong>Zhaoheng Zheng</strong>, Hao-Tian Zhang, Fang-Lue Zhang, and Tai-Jiang Mu
<br> Computational Visual Media, 2017<br>
<a href="https://cs.stanford.edu/~haotianz/research/clothes_changing/clothes_changing.pdf" class="btn btn--success">Paper</a></p>

<p hidden><script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=WuTycU_gptD1_uRJMJF-BV4Q0VudvsyDQpgvA3okEYs&cl=ffffff&w=a"></script></p>
