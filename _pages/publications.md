---
layout: archive
title: "Publications"
permalink: /publications/
author_profile: true
---


<!-- You can also find my publication list on <u><a href="https://scholar.google.com/citations?user=36e4ADAAAAAJ&hl=en">my Google Scholar profile</a>.</u> -->


<h2> 2023 </h2>

<p><u>Large Language Models are Good Prompt Learners for Low-Shot Image Classification</u><br>
Zhaoheng Zheng, <strong>Jingmin Wei</strong>, Xuefeng Hu, Haidong Zhu, and Ram Nevatia
<br> [Under Review] <br>
<!-- <a href="https://arxiv.org/abs/2305.16681" class="btn btn--success">Paper</a></p> -->
Abstract: Low-shot image classification, where training images are limited or inaccessible, has benefited from recent progress on pre-trained vision-language (VL) models with strong generalizability, e.g. CLIP. Prompt learning methods built with VL models generate text features from the class names that only have confined class-specific information. Large Language Models (LLMs), with their vast encyclopedic knowledge, emerge as the complement. Thus, in this paper, we discuss the integration of LLMs to enhance pre-trained VL models, specifically on low-shot classification. However, the domain gap between language and vision blocks the direct application of LLMs. Thus, we propose LLaMP, Large Language Models as Prompt learners, that produces adaptive prompts for the CLIP text encoder, establishing it as the connecting bridge. Experiments show that, compared with other state-of-the-art prompt learning methods, LLaMP yields better performance on both zero-shot generalization and few-shot image classification, over a spectrum of 11 datasets.

<h2> 2021 </h2>
<p><u>GDN: A Stacking Network Used for Skin Cancer Diagnosis</u><br>
<strong>Jingmin Wei</strong>, Haoyang Shen, Ziyi Wang, Ziqian Zhang
<br> ICSPS 2021<br>
<a href="https://doi.org/10.1117/12.2631455" class="btn btn--success">Paper</a></p>